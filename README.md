# Gradual Growth vs Naive Learning in Continuous Learning Scenarios

This repository contains the implementation and experiments for comparing **gradual growth methods**, specifically the G_stack approach, with **naive models trained from scratch**. The project focuses on addressing **catastrophic forgetting** and improving model performance in continuous learning scenarios.

[b]keywords:[/b] probabilistic continuous learning, deterministic and probabilistic fine tuning, catastrophic forgetting, model growth, g_stack, FiLM-Ensemble
---

## ğŸš€ Project Objectives

1. Compare the effectiveness of G_stack and naive models in continuous learning tasks.
2. Investigate the impact of **deterministic** and **probabilistic fine-tuning** on model performance.
3. Analyze how these methods mitigate catastrophic forgetting when revisiting the same task and dataset over time.

---

## ğŸ“‚ Repository Structure

```
gradual-growth-vs-naive-learning/
â”œâ”€â”€ data/                      # Data storage
â”‚   â”œâ”€â”€ raw/                   # Raw datasets
â”‚   â”œâ”€â”€ results/               # Experiment outputs
â”œâ”€â”€ src/                       # Source code
â”‚   â”œâ”€â”€ models/                # Model implementations
â”‚   â”œâ”€â”€ utils/                 # Utilities for data handling and metrics
â”‚   â”œâ”€â”€ training/              # Training and evaluation scripts
â”œâ”€â”€ experiments/               # Experiment configurations and logs
â”œâ”€â”€ notebooks/                 # Jupyter notebooks for analysis
â”œâ”€â”€ tests/                     # Unit tests
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ environment.yml            # Conda environment file (optional)
â””â”€â”€ README.md                  # Project overview
```

---

## ğŸ“¦ Installation

### Prerequisites
- Python 3.8 or later
- Recommended: [Conda](https://docs.conda.io/en/latest/) for environment management

### Setup
1. Clone the repository:
   ```bash
   git clone https://github.com/your-username/growth-vs-forgetting.git
   cd growth-vs-forgetting
   ```
2. Install dependencies:
   - Using pip:
     ```bash
     pip install -r requirements.txt
     ```
   - Using Conda:
     ```bash
     conda env create -f environment.yml
     conda activate gradual-growth-env
     ```

---

## ğŸ› ï¸ Usage

### 1. Train Models
#### Train G_stack Model:
```bash
python src/training/ftune_g_stack.py --config experiments/configs/g_stack_config.json
```

#### Train Naive Model:
```bash
python src/training/ftune_naive.py --config experiments/configs/naive_model_config.json
```

### 3. Evaluate Models
```bash
python src/training/evaluate.py --model g_stack --metrics ECE
```

### 4. Visualize Results
Use the provided Jupyter notebooks:
```bash
jupyter notebook notebooks/results_visualization.ipynb
```

---

## ğŸ“Š Key Features

- **Gradual Growth (G_stack)**: Plese refer to https://llm-stacking.github.io/
- **Naive Model Training**: Baseline for evaluating G_stackâ€™s effectiveness.
- **Deterministic and Probabilistic Fine-Tuning**: Explore their impact on calibration and robustness.
- **Metrics**: Evaluate catastrophic forgetting, Expected Calibration Error (ECE), and task accuracy.

---

## ğŸ“ˆ Results
Detailed results, including metrics like ECE and diversity scores, are stored in the `data/results/` directory and visualized in the `notebooks/`.

---

## ğŸ“§ Contact
For questions or collaboration, contact [Ege SÃ¼alp](mailto:e.sualp@campus.lmu.de).



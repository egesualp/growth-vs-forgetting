{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'HuggingFaceLM' from 'lm_eval.models.huggingface' (C:\\Users\\esual\\Documents\\lmu_repos\\lm-evaluation-harness\\lm_eval\\models\\huggingface.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlm_eval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhuggingface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceLM\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlm_eval\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m evaluator\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Path to your local checkpoint (which contains both model and tokenizer files)\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'HuggingFaceLM' from 'lm_eval.models.huggingface' (C:\\Users\\esual\\Documents\\lmu_repos\\lm-evaluation-harness\\lm_eval\\models\\huggingface.py)"
     ]
    }
   ],
   "source": [
    "from lm_eval.models.huggingface import HuggingFaceLM\n",
    "from lm_eval import evaluator\n",
    "\n",
    "# Path to your local checkpoint (which contains both model and tokenizer files)\n",
    "model_path = \"./path/to/your/local/checkpoint\"\n",
    "\n",
    "# Instantiate your model wrapper.\n",
    "# You can specify the device (\"cuda\" or \"cpu\") and max_length as needed.\n",
    "model = HuggingFaceLM(\n",
    "    model_name_or_path=model_path,\n",
    "    tokenizer_name_or_path=model_path,\n",
    "    device=\"cuda\",    # or \"cpu\" if you don't have a GPU\n",
    "    max_length=1024   # adjust as needed\n",
    ")\n",
    "\n",
    "# Define the tasks you want to evaluate.\n",
    "# (See the lm-evaluation-harness repo for the full list of supported tasks.)\n",
    "tasks_to_eval = [\"lambada_openai\", \"hellaswag\"]\n",
    "\n",
    "# Run the evaluation.\n",
    "results = evaluator.evaluate(model, tasks=tasks_to_eval)\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "YAML_boolq_string = \"\"\"\n",
    "task: demo_boolq\n",
    "dataset_path: super_glue\n",
    "dataset_name: boolq\n",
    "output_type: multiple_choice\n",
    "training_split: train\n",
    "validation_split: validation\n",
    "doc_to_text: \"{{passage}}\\nQuestion: {{question}}?\\nAnswer:\"\n",
    "doc_to_target: label\n",
    "doc_to_choice: [\"no\", \"yes\"]\n",
    "should_decontaminate: true\n",
    "doc_to_decontamination_query: passage\n",
    "metric_list:\n",
    "  - metric: acc\n",
    "\"\"\"\n",
    "with open(\"boolq.yaml\", \"w\") as f:\n",
    "    f.write(YAML_boolq_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntask: demo_boolq\\ndataset_path: super_glue\\ndataset_name: boolq\\noutput_type: multiple_choice\\ntraining_split: train\\nvalidation_split: validation\\ndoc_to_text: \"{{passage}}\\nQuestion: {{question}}?\\nAnswer:\"\\ndoc_to_target: label\\ndoc_to_choice: [\"no\", \"yes\"]\\nshould_decontaminate: true\\ndoc_to_decontamination_query: passage\\nmetric_list:\\n  - metric: acc\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YAML_boolq_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: LOGLEVEL=DEBUG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python was not found; run without arguments to install from the Microsoft Store, or disable this shortcut from Settings > Manage App Execution Aliases.\n"
     ]
    }
   ],
   "source": [
    "%env LOGLEVEL=DEBUG\n",
    "!python3 lm-evaluation-harness/main.py \\\n",
    "    --model hf-causal-experimental \\\n",
    "    --model_args pretrained=\"checkpoint-1547/checkpoint-1547\" \\\n",
    "    --tasks  piqa,boolq,winogrande,hellaswag,mathqa,mutual \\\n",
    "    --device cuda:0 \\\n",
    "    --output_path results.txt \\\n",
    "    --no_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python lm-evaluation-harness/main.py --model hf-causal-experimental --model_args pretrained=\"growth-vs-forgetting\\notebooks\\checkpoint-1547\\checkpoint-1547\" --tasks  piqa,boolq,winogrande,hellaswag,mathqa,mutual --device cuda:0 --output_path results.txt --no_cache"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clearning2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

model_name_or_path: "llm-stacking/StackLLM_410M_750BToken"  # Model name or path
#model_name_or_path: "gpt2"
#dataset: "../../data/raw/wiki_auto/simplification_1_train.json"  # Dataset to use
dataset: "../../data/preprocessed/simp_wiki_auto.json"
dataset_format: "input-output"                              # Dataset format
output_dir: "../models/GStack_410M_T750B_wiki_auto_2" 
trust_remote_code: false
max_length: 512
max_eval_samples: 256 # Optional based on task evaluation setup
#max_train_samples: 200
overwrite_output_dir: true
do_train: false
do_eval: true
do_predict: false
evaluation_strategy: "steps"
per_device_train_batch_size: 16
per_device_eval_batch_size: 16
gradient_accumulation_steps: 4
eval_accumulation_steps: 8
dataloader_num_workers: 8
learning_rate: 2e-5
weight_decay: 0.0
num_train_epochs: 3
max_steps: -1
lr_scheduler_type: "constant"
logging_steps: 10
save_strategy: "steps"
save_steps: 500
eval_steps: 10
save_total_limit: 3
seed: 42
#report_to: wandb                                    # Reporting integrations (e.g., wandb)
fp16: true
dataloader_pin_memory: true
task: 'Simp'
predict_with_generate: true 
generation_max_length: 256

model_name_or_path: "llm-stacking/StackLLM_410M_750BToken" 
dataset: "../../data/preprocessed/simp_wiki_auto_new.json"
dataset_format: "input-output"
output_dir: "../models/stack_410m_m0/test"
task: 'Simp'
predict_with_generate: true

# Generation parameters
num_beams: 5                   # Keeps beam search for high-quality generation
temperature: 0.7               # Lower temperature for more deterministic outputs
top_k: 50                      # Retain for diversity control
top_p: 0.9                     # Retain for nucleus sampling
length_penalty: 0.8            # Penalizes overly long outputs
generation_max_length: 256     # Reduced to save memory if shorter generations suffice
remove_unused_columns: False
# Execution controls
do_train: false
do_eval: true
do_predict: true

# Performance optimizations
fp16: true                      # Use mixed precision for GPU memory optimization
gradient_checkpointing: false   # Not needed for inference (saves CPU overhead)
use_cpu: false                  # Ensure GPU usage unless no GPU is available
logging_steps: 50               # Increase to reduce logging overhead
save_strategy: "no"             # No saving required for inference
evaluation_strategy: "no"       # No evaluation steps during prediction
train_on_source: false

# Batch sizes and memory usage
per_device_train_batch_size: 16
per_device_eval_batch_size: 16
gradient_accumulation_steps: 4
eval_accumulation_steps: 8
dataloader_num_workers: 8

# Model limits
max_length: 256                 # Align with generation_max_length for consistency

# Config file for fine-tuning with finetune.py

model_name_or_path: "llm-stacking/G_random_width"  # Pretrained model name or path
trust_remote_code: false          # Use remote code if the model requires it
eval_dataset_size: 1024           # Size of the evaluation dataset

source_max_len: 1024              # Maximum input sequence length
target_max_len: 256               # Maximum output sequence length
dataset_path: "../../data/preprocessed/simplification_man.json"
output_dir: "../models"           # Directory for logs and checkpoints
overwrite_output_dir: false       # Overwrite existing output directory

do_train: true                    # Enable training
do_eval: false                    # Enable evaluation
do_predict: false                 # Enable prediction

evaluation_strategy: "steps"     # Evaluation strategy: no, steps, or epoch
per_device_train_batch_size: 16   # Training batch size per GPU
per_device_eval_batch_size: 8     # Evaluation batch size per GPU
gradient_accumulation_steps: 1    # Steps for gradient accumulation
learning_rate: 0.0002             # Learning rate
weight_decay: 0.0                 # Weight decay
adam_beta1: 0.9                   # Beta1 for Adam optimizer
adam_beta2: 0.999                 # Beta2 for Adam optimizer
adam_epsilon: 1e-8                # Epsilon for Adam optimizer
max_grad_norm: 0.3                # Max gradient norm
num_train_epochs: 3               # Number of training epochs
max_steps: 10                  # Max optimization steps
lr_scheduler_type: "constant"    # Learning rate scheduler type
warmup_ratio: 0.03                # Warmup ratio
warmup_steps: 0                   # Warmup steps

logging_dir: "../logs"            # Directory for logs
logging_strategy: "steps"        # Logging strategy
logging_steps: 10                 # Logging frequency
save_strategy: "steps"           # Save strategy
save_steps: 250                   # Save frequency
save_total_limit: 40              # Maximum number of checkpoints to keep

fp16: false                       # Enable FP16 precision
bf16: false                       # Enable BF16 precision
seed: 42                          # Random seed

dataloader_num_workers: 0         # Number of workers for DataLoader
dataloader_pin_memory: true       # Pin memory for DataLoader

report_to: wandb                # Reporting options (e.g., wandb)
predict_with_generate: false      # Use generate method during prediction
generation_max_length: 256        # Max generation length

gradient_checkpointing: true      # Enable gradient checkpointing

# Add any additional parameters as needed below.
